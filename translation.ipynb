{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26711,
     "status": "ok",
     "timestamp": 1745400297188,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "oyAZ8HDf9ixe",
    "outputId": "1cef1b60-df65-412a-e48d-76f80b6a8eb4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2914,
     "status": "ok",
     "timestamp": 1745400301203,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "xhEB1_V-9ogP"
   },
   "outputs": [],
   "source": [
    "# # Define the path to your dataset folder\n",
    "# data_path = \"/\"\n",
    "\n",
    "# Load datasets directly from Google Drive\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "val_df = pd.read_csv(\"val_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10062,
     "status": "ok",
     "timestamp": 1745400680534,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "I9nuJ4UDDmzN",
    "outputId": "e5dda40b-7b2b-40f8-c78f-7f328e913c50"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1745400314798,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "lBhSehgg9w-J",
    "outputId": "f85e47b1-86c9-47f8-fa06-8f527593664f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                English             Italian\n",
      "0         MEMBRANE UD-1       MEMBRANA UD-1\n",
      "1      PLASTIC BASE LT8   BASE CABINATO LT8\n",
      "2      COMMUTATOR 6 POS   COMMUTATORE 6 POS\n",
      "3     MOTOR CASING K120  CARTER MOTORE K120\n",
      "4  SCREW F3,5X13 N11725                VITE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train_data.csv')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110470,
     "status": "ok",
     "timestamp": 1745400439224,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "HnrL6pUB90DW",
    "outputId": "0eecf651-2b90-47be-ebd7-1c59562caa7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: portalocker in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: networkx in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets sentencepiece sacrebleu evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "b45c39cb8a3640e1a3ed3e0496c21e16",
      "46d0519459be41aca2f0be520d9e679c",
      "ae04546eb48c4aee98a936cb96d8d0e3",
      "48689fba875b4080b969d2d5c278a126",
      "e74aa2c64de245b7809a2f9a1f15fbc5",
      "88ee3b026c1d4df8887a5b050d4735a6",
      "1ccf6cfb15744a35b5063d059bddd7d5",
      "0ceeb5c445f34a5ab9e325fef8103639",
      "6afb177ff2c04dea8d4698b69a025826",
      "6e5c5b68d13a4e9b92ff480984c15bb2",
      "4200dad77522452d998ae0df7325e894",
      "6a5467bb83104e1fb8cf30272fbfd721",
      "746ee900a6d44bdc91698987a35ba517",
      "bd83710633324e0cacaa1125c208f156",
      "839f150a250a43479fed1c3efde7553f",
      "998f7e786bf54628b78199491ce924cf",
      "c41d2b6fb2ba4e2497ea4cb6d88aecf6",
      "f043831f057047b2b2448f35f6a09972",
      "921bb8e8d5f2429ead9ded31b9e09504",
      "4e7dda0c09164b979fb59554c2012bd0",
      "207e58ba94aa4ae09d9b275f68adddca",
      "b249377cb5fd47e593558cc3773af065",
      "4eaf54c7f98644799abf1d602f5705d4",
      "ad19d8ae14fd4470a813c0682b014c5a",
      "e442b650b558480bb9239f317e4b627d",
      "edaf105037f44bd0b2a8d66a2ad97cd8",
      "07b229ed27284b39823dea03d445aefd",
      "1ec9c0e8ba134370aa341f35a91e5a8e",
      "f3c4910773bb4c72a595217c8dc15364",
      "5ecef6ea6371435ebae3d55e645976bc",
      "a4043afa4f0a4226908157084a2b6336",
      "6a80171689344294926ae8575b973840",
      "4e07fe385b34464a8a0789c5d98c756a",
      "6cd7174af0254a55a01f693b7cc12536",
      "d088dac7c08a459ebf1e70186cdffd1d",
      "5ee9eb7eb558442995d4fcf917270dbe",
      "33ee87ec926a4ee39db61e46c01adcfb",
      "6678b647b93b40eeb91237b36aeef76c",
      "37ac976f93aa4df9a1a54ace2b2604db",
      "00c88ad51c9d40b6a2fbd760147c6cc8",
      "7b54f7f36387497ba1aa33c1684a8275",
      "53fd68cfbb2b417ab56cd17e58653f0d",
      "c2d5df5bf7ad424e96c0c1f766e840b3",
      "7362b72b61d544108075a0e14943ec95",
      "cb5811fe39594855b060f43013b6fd09",
      "84ee9ded81c546048ee07f4763e4fb7d",
      "8e23880315724ad4a1e0e9ee14f92aa6",
      "dde8147366b6424ea89f4a3bbeeb5658",
      "6f0ee5ede3f04d8db3209e00bda2e411",
      "1a21dfc2e6824aedb0522a88bddef94c",
      "7650aa2e817a46318e41f11e934cd0e8",
      "2e742df338b24dc795da4b6cdb14fd2a",
      "b34c02b82e5241f5ac359330e502e6e6",
      "5c2c92e99e414a40acaa18f36ac29f7e",
      "d5616cffa0fa4782a2fef80f95850e53",
      "c1fe440eaaa440d1b801f04d5d8471ea",
      "1bdfbf52e5514c89a6aa806e9947b1fa",
      "397d26f61fec49afb2c29e7bcec6f135",
      "db6b402ec227472c866a2c63987f9667",
      "1c83968a9d674f9e90629db7b72d67f9",
      "3ac68e71e9f54526852c6c3d3b3743c5",
      "03189a2ff6a648cc89cfd0aa1b451e9f",
      "9b5c084d4e1b4170b62b1fd52b3fded4",
      "9c272b3bef6743debe28df507ecb59ba",
      "039c642ba96e43df952634670cfa3a1b",
      "d064561daf0a400ea7129afed610832b",
      "cb9f292b5e1f49fa8fe40f35846e459f",
      "978aa1988653485592a16663322d4f98",
      "ff890a1562f746648928653eb6ac0632",
      "906d99d3f3a24f8da501db0439cd7dd0",
      "8dd8c849819e4fcea28a41eefac4117d",
      "955d16753cc94d2da904dabf72474385",
      "f430d1061c67460e9d9dde9095906b07",
      "f3ef2397fa8c4f19b33a688cd4ab649d",
      "727619418118405a924480779006f25d",
      "56a1f055b93149cd944ee72c573e49d3",
      "686d605304394701abd98e56750c1e2d"
     ]
    },
    "executionInfo": {
     "elapsed": 18426,
     "status": "ok",
     "timestamp": 1745400497317,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "9scU5pz896th",
    "outputId": "ead20c5a-e931-4f3c-9bb3-c22c54770c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 300\n",
      "Validation size: 50\n",
      "Test size: 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "\n",
    "# # Load the datasets\n",
    "# train_df = pd.read_csv('/content/drive/MyDrive/translation_data/train_data.csv')\n",
    "# val_df = pd.read_csv('/content/drive/MyDrive/translation_data/val_data.csv')\n",
    "# test_df = pd.read_csv('/content/drive/MyDrive/translation_data/test_data.csv')\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Reduce dataset\n",
    "train_dataset = train_dataset.select(range(min(300, len(train_dataset))))\n",
    "val_dataset = val_dataset.select(range(min(50, len(val_dataset))))\n",
    "test_dataset = test_dataset.select(range(min(50, len(test_dataset))))\n",
    "\n",
    "# Verify the new sizes\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sheiksah\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'facebook/mbart-large-50-many-to-many-mmt'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/mbart-large-50-many-to-many-mmt' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MBart50TokenizerFast\n\u001b[1;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MBart50TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./cache\u001b[39m\u001b[38;5;124m'\u001b[39m, force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2051\u001b[0m     )\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'facebook/mbart-large-50-many-to-many-mmt'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/mbart-large-50-many-to-many-mmt' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_checkpoint, cache_dir='./cache', force_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'facebook/mbart-large-50-many-to-many-mmt'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/mbart-large-50-many-to-many-mmt' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MBart50TokenizerFast\n\u001b[0;32m      3\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/mbart-large-50-many-to-many-mmt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m MBart50TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_checkpoint, src_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m, tgt_lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit_IT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_data\u001b[39m(example):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2044\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2047\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2049\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2050\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2051\u001b[0m     )\n\u001b[0;32m   2053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'facebook/mbart-large-50-many-to-many-mmt'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'facebook/mbart-large-50-many-to-many-mmt' is the correct path to a directory containing all relevant files for a MBart50TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import MBart50TokenizerFast\n",
    "\n",
    "model_checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_checkpoint, src_lang=\"en_XX\", tgt_lang=\"it_IT\")\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def tokenize_data(example):\n",
    "    inputs = tokenizer(example['English'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(example['Italian'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_data, batched=True, remove_columns=['English', 'Italian'])\n",
    "tokenized_val = val_dataset.map(tokenize_data, batched=True, remove_columns=['English', 'Italian'])\n",
    "tokenized_test = test_dataset.map(tokenize_data, batched=True, remove_columns=['English', 'Italian'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3JKdDol9-WN"
   },
   "source": [
    "# Setup Data Collator and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "cb421d71728f418484683a83059054b4",
      "2fc7e35be2b14b5eb9d7b51ed297f03d",
      "8c7b850c449d42629f472288f82aa0e4",
      "777e05ca8ee24a148d83ed79ecf2212b",
      "881e536b25344f8aa75a0542c01e35f0",
      "2803285cdfc647e1ac27301a210f01b1",
      "f23205c38c0743579c955ccaa1a66488",
      "dbd31b39592b4708bc35d10a14764001",
      "6e16e8808b794fd48a909b2da3ae1381",
      "1aec1e00d75c438586036002f3de311c",
      "294785f70252474d892010c96837b0b2"
     ]
    },
    "executionInfo": {
     "elapsed": 21167,
     "status": "ok",
     "timestamp": 1745400565968,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "0w1257RKB9iF",
    "outputId": "812d4f9a-6a9d-4706-e0bc-defdef532817"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_checkpoint, padding=True)\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n",
    "    return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKASzv6jCKCT"
   },
   "source": [
    "#Initialize the mBART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c6ce3f58f1f94a20917b1b0f77727aee",
      "8d14e89de84f4cc5bfab6582257775a7",
      "8824c3636cc3484bbd1d6624bf23f5b7",
      "a1f7af795c8b4cabb1370889d89c5502",
      "00a078df8c164199b23a6bf01a4fc011",
      "689aff72a74d4e81aea377f11fd23af0",
      "933f30fa17b4433f84c2630c83812f18",
      "de4175cbe470419386f3f423f4f93b4f",
      "db88c5d946a447c09e18f767dd6516f6",
      "e42840b0aeee4341bb81ad78dc66f428",
      "b14424c2da014721b261a4ddde9db943",
      "2b7df9fe2f7b4ffea7ce63f8e59e4261",
      "9f642e422e8e4b17be735f4775bda299",
      "ced38704f8a14ed0a4eb14f493a38cac",
      "3aded1db82c040c6bd025de8a8f1d909",
      "e5715c6caee74bf48826ee576619e4d4",
      "f5470ff33c854e28aff6e7646be0152b",
      "24987d0a76da4d70bcce00bb72b46f64",
      "636da80febb6488288c98436582a380f",
      "16013345a1ec430d9f03e7062e3326ea",
      "f432a40484974c5ba76076a1855078f7",
      "1162c13e4251452baece26a505759d10"
     ]
    },
    "executionInfo": {
     "elapsed": 25019,
     "status": "ok",
     "timestamp": 1745400603131,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "bdmcNRS7CLNi",
    "outputId": "fff3ed7a-db0b-47aa-d14e-46f0c3f51bbc"
   },
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zyv7-HWCOrr"
   },
   "source": [
    "#Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2356,
     "status": "ok",
     "timestamp": 1745400607055,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "53RZN0AQCQEd"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "       output_dir=\"mbart-eng-it-domain-specific\",\n",
    "       eval_strategy=\"epoch\",\n",
    "       save_strategy=\"epoch\",\n",
    "       learning_rate=2e-5,\n",
    "       per_device_train_batch_size=4,  #  batch size\n",
    "       per_device_eval_batch_size=4,  #  batch size\n",
    "       num_train_epochs=3,\n",
    "       weight_decay=0.01,\n",
    "       predict_with_generate=True,\n",
    "       save_total_limit=2,\n",
    "       fp16=True,\n",
    "       report_to=None\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGGnq1jzCdWy"
   },
   "source": [
    "# Initialize the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5380,
     "status": "ok",
     "timestamp": 1745400619584,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "oewzSZvLCfFL",
    "outputId": "4e2fc218-b996-456b-bea3-60eac482c582"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-cd3df47410e1>:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHCKzciLCtG6"
   },
   "source": [
    "#Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjwrfdVPCgt-"
   },
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZg0dV7sDpgx"
   },
   "source": [
    "# Disable wandb to train free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4GsLLCdCw6j"
   },
   "source": [
    "#Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "error",
     "timestamp": 1745391436425,
     "user": {
      "displayName": "sahar sheikhi",
      "userId": "12536850176286676538"
     },
     "user_tz": -120
    },
    "id": "Vg5uCCvOC14a",
    "outputId": "bb576d23-ac85-49eb-ba21-7a0f6609e669"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4f2f6054a11>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"BLEU score on test set: {eval_results['eval_bleu']:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate(tokenized_test)\n",
    "print(f\"BLEU score on test set: {eval_results['eval_bleu']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pM7u4QxC220"
   },
   "source": [
    "# Save model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R58y3IDNC5DU"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"mbart-eng-it-trained\")\n",
    "tokenizer.save_pretrained(\"mbart-eng-it-trained\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWxgmgqVC8Ii"
   },
   "source": [
    "#Inference - Testing translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6mmOB8WC9b7"
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# translator = pipeline(\n",
    "#     \"translation\",\n",
    "#     model=\"mbart-eng-it-trained\",\n",
    "#     tokenizer=\"mbart-eng-it-trained\",\n",
    "#     src_lang=\"en_XX\",\n",
    "#     tgt_lang=\"it_IT\"\n",
    "# )\n",
    "\n",
    "# sentence = \"This is a domain-specific sentence for translation.\"\n",
    "# translated_sentence = translator(sentence)[0]['translation_text']\n",
    "# print(f\"Translated sentence: {translated_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUrcYUA5RSva6JFlkTXZdH",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
